\section{Spectral clustering}
\label{sec:spectralclustering}
% Basic
Spectral clustering algorithm is a graph-based clustering algorithm. 
It treats data points are represented as vertices and constructs an affinity matrix after measuring pairwise similarity and distance between points to form edges. 
After that, it takes top eigenvectors of an eigenvectors of an affinity matrix, and then uses graph cut algorithm to form clusters. 
In our implementation, we follow Shi's multiclass normalized cut algorithm \cite{jianbo03}. 
The assumptions to the methods are that \begin{inparaenum}[\itshape a\upshape)]
%It involves taking top eigenvectors of an affinity matrix, which is transformed by calculating the similarity between objects in dataset, and then use graph cut algorithm. 
\item minimizing normalized cut is equivalent to minimizing the probability that a random walk on the graph.
\item the larger the gap between $\lambda_{k}$ and $\lambda_{k+1}$, the closer the piecewise-constant eigenvectors will be.
\end{inparaenum}
In this section, I describe the detail of the spectral clustering algorithm. 
Most of them can be found in \cite{ulrike07}. 

%\subsection{Normalized Laplacian Matrix}
Spectral clustering algorithms attempt to find $k$ subsets of vertices which have the property that vertices within a cluster are similar to each other than to vertices in other clusters. 
In order to find subsets, spectral clustering algorithm needs the normalized Laplacian $n \times n$ matrix $L$. 

Let $A$ is the affinity matrix $n \times n$ dimensions. It is defined as 
\begin{equation}
A_{i,j} = \left\{ 
  \begin{array}{l l}
    sim(x_i, x_j) & \quad \text{for $x_j$ is in neighbourhood of $x_i$}\\
    0 & \quad \text{otherwise}
  \end{array} \right.
\end{equation}
This means $A_{i,j}$ is the pairwise similarity only if they are neighbourhood. 
In this paper, I use nearest 8-neighborhood to construct graph by inserting edges between a node and its nearest 8-neighbours. 
For example, if there is a graph $G$ which has 100 nodes and they are fully connected, each node of nearest 8-neighborhood graph $\hat{G}$ will be connected to only 8 nodes that have highest weight among them. 

Note that we are assuming that the graph is undirected which makes $A$ symmetric and a weight of an edge is a non-negative measure of similarity between the two vertices. 
Higher weight implies greater similarity. 
In the paper, I use cosine similarity for similarity function $sim$. It is discussed in Section~\ref{subsec:normalabnormalsimilarity}. 

Let $D$ be the diagonal degree matrix $n \times n$ dimensions where $d_{i,i} = \sum_{j} A_{i,j}$. 
%and $W$ be the weighted adjacency matrix, such that $W_{i,i} = 0$ and $w_{i,j}$ is the weight of the edge between $v_i \in V$ and $v_j \in V$. 
Then we can get normalized Laplacian matrix $L$ as follows : 
\begin{equation}
L = D^{-1/2} A D^{-1/2}
\end{equation}
%where $D$ is the diagonal degree matrix $d_{ii}$ is the detree of vertex $i$ defined as $d_{ii} = \sum_{j=1}^n$ and $W$ is the weighted adjacency matrix, such that $W_{ii} = 0$ and $w_{ij}$ is the weight of the edge between $v_i \in V$ and $v_j \in V$. 

%\subsection{Algorithm}
A spectral clustering builds embedded space from the eigenvectors corresponding to the $k$ eigenvalues before it does $k$-way clustering. 
We can find the value of $k$ that maximizes the eigengap which finds a maximum difference between consecutive eigenvalues by following equation. 
\begin{equation}
\operatorname*{arg\,max}_{k} \Delta_k = | \lambda_k - \lambda_{k-1} | 
\end{equation}
This is important to understand that $k$ is the number of top eigenvalues determined by eigengap and is also the number of clusters. 

From $L$, we can get the eigenvalues and eigenvectors of Laplacian matrix, and we can apply known outlier detection techniques \cite{knorr00} to the reduced $n \times k$ space to produce $k$ clusters. 
%we need to find the eigenvalues and eigenvectors of Laplacian matrix to decompose do this. 
%After we compute normalized graph Laplacian matrix, we can decompose the matrix. 
%After we find them, we can build embedded space from the eigenvectors corresponding to the $k$ eigenvalues given by eigengap algorithm. 
%After that, we can apply known outlier detection techniques \cite{knorr00} to the reduced $n \times k$ space to produce $k$ clusters. 
\begin{figure}[ht]
\begin{mdframed}
\begin{enumerate}
\item[Input] : Similarity matrix $S \in R^{n \times n}$, number $k$ of clusters. 
\item[Step 1] : Construct the normalized graph Laplacian $L = D^{-1/2} A D^{-1/2}$.
%Construct the normalized graph Laplacian $L = D^{-1/2} A D^{-1/2} - W$, using the weighted $n \times n$ adjacency matrix $W$, where $w_{ij}$ is a function of $S_{ij}$ that gives non-negative weights. \\
\item[Step 2] : Find the $k$ eigenvectors $u_1, \cdots, u_k$ corresponding to the smallest $k$ eigenvalues that solve the generalized eigenvector problem $L u = \lambda D u$ 
\item[Step 3] : Let $U = R^{n \times k}$ be the matrix containing the eigenvectors $u_i$ as columns, and let $y_i \in R^k$ be the $i$th row of $U$.
\item[Step 4] : Cluster the points $(y_i)_{i=1,\cdots,n}$ in $R^k$ using $k$-means clustering into clusters $C_1,\cdots,C_k$.
\item[Output] : Clusters $C_1, \cdots, C_k$
\end{enumerate}
\end{mdframed}
\caption{Overview of multiclass spectral clustering algorithm according to Shi}
\end{figure}

A spectral clustering has three issues. 
First of all, eigengap does not pick $k$ value correctly if there is too many noises. 
We can check whether the result of eigengap is fine or not by threshold. 
In this paper, I use $\sqrt{n}$ where $n$ is the number of data points as threshold. 
Secondly, the similarity function can affect the accuracy a lot. 
I describe how to train similarity score function in section~\ref{subsec:normalabnormalsimilarity}. 
Thirdly, we need to choose proper clustering method. 
I follow $k$-class clustering approach as I mentioned before because I found recursive bipartite approach is not applicable for real data which has lots of noise. 
%For clustering method, there is two ways to do it. 
%First is a $k$-class clustering \cite{jianbo03} and the other is one by one iteratively until it makes $k$ clusters. 
%The problem of latter one is that it is too weak for a noise. 
%Also the number of cluster $k$ from its eigengap does not always best when the data have considerable noises. 
%Therefore I follow multi-class approach since iterated one-to-one clustering is not applicable. %in this problem because of the limitation of eigensolver's sensitivity. 
% I followed both way but one-by-one is sometimes really bad.
