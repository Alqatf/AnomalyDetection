\section{Spectral clustering}
\label{sec:spectralclustering}
% Basic
Spectral clustering algorithm is a graph-based clustering algorithm of $n$ data points. 
It make use of an $n \times n$ affinity matrix which is constructed after measuring pairwise similarity and distance between data points to form edges. 
Unlike the other clustering algorithms, it can determine the number of clusters $k$ by using eigengap algorithm. 
%It make use of similarity matrix which represents data points as vertices. 
After that, it takes $k$ eigenvectors of corresponding to the $k$ largest eigenvalues of an affinity matrix as columns, 
then uses $k$-way clustering algorithm in $k$ dimensions. 
In short, it performs dimensionality reduction since $k < n$ then produces clusters in fewer dimensions. 
Note that the number of cluster $k$ is the same as the number of column $k$ of reduced space. 

The assumptions to the methods are that \begin{inparaenum}[\itshape a\upshape)]
%It involves taking top eigenvectors of an affinity matrix, which is transformed by calculating the similarity between objects in dataset, and then use graph cut algorithm. 
\item minimizing normalized cut is equivalent to minimizing the probability that a random walk on the graph.
\item the larger the gap between eigenvalues of the graph, %$\lambda_{k}$ and $\lambda_{k+1}$, 
the closer the piecewise-constant eigenvectors will be
\end{inparaenum}. \newline 
%In this section, I describe the detail of the spectral clustering algorithm. More details can be found in \cite{ulrike07}.\newline
%In Section~\ref{subsec:eigengap}, I describes the eigengap algorithm.\newline
%In Section~\ref{subsec:multiclass}, I describes the multiclass spectral clustering algorithm.

%So it may be applied to image data but not to real network connection data. 
% Although its performance is generally good, it is weak at noise. 
% In what way they differ from what we do here
% Why is our approach better than what has done before. does it solve a partly new aspect of the problem or does it simply perform better
%It will measure pairwise similarity of data points. %like the other spectral clustering. 
%In our implementation, we follow Shi's multiclass normalized cut algorithm \cite{jianbo03}. 

\subsection{Eigengap algorithm}
\label{subsec:eigengap}
% Important concept - k -  to a reviewer first 
The eigengap is a difference between two consecutive eigenvalues. 
We can find the value of $k$ that maximizes the eigengap by following equation. 
\begin{equation}
\Delta_{max} = \operatorname*{arg\,max}_{k} \Delta_k, \text{~~} \Delta_k = | \lambda_k - \lambda_{k-1} |
\end{equation}
where $\lambda$ is an eigenvalue and $k$ is a desirable choice of the number of clusterings \cite{ulrike07}.
This is important to understand that $k$ is the number of top eigenvalues determined by eigengap and is also the number of clusters. 

\subsection{Multiclass spectral clustering algorithm}
\label{subsec:multiclass}
%\subsection{Algorithm}
%\subsection{Normalized Laplacian Matrix}
% Then, spectral clustering
Spectral clustering algorithms attempt to find $k$ subsets of data points which have the property that data points within a cluster are similar to each other than to data points in other clusters. 
%In order to do this, a spectral clustering builds embedded space from the eigenvectors corresponding to the $k$ eigenvalues before it does $k$-way clustering. 
%subsets, spectral clustering algorithm needs the normalized Laplacian $n \times n$ matrix $L$. 

Let $A \in \mathbb{R}^{n \times n}$ is the affinity matrix where $n$ is the number of data points. It is defined as 
\begin{equation}
\label{eq:sim}
A_{i,j} = \left\{ 
  \begin{array}{l l}
    sim(x_i, x_j) & \quad \text{for $x_j$ is a data point in neighbourhood of $x_i$}\\
    0 & \quad \text{otherwise}
  \end{array} \right.
\end{equation}
where neighbourhood means 8-nearest neighbourhood that is all 8 closest data points around the point $x_i$
and $sim$ is a cosine similarity function. 
%In the paper, I use cosine similarity for similarity function $sim$. It is discussed in Section~\ref{subsec:normalabnormalsimilarity}. 
%That is, edges of $A$ only if they are neighbourhood. 
%In short $A_{i,j}$ is the pairwise similarity only if they are neighbourhood. 
%I use nearest 8-neighborhood to construct graph by inserting edges between a node and its nearest 8-neighbours. 
%For example, if there is a graph $G$ which has 100 nodes and they are fully connected, each node of nearest 8-neighborhood graph $\hat{G}$ will be connected to only 8 nodes that have highest weight among them. 
Note that we are assuming that the graph is undirected which makes $A$ symmetric and a weight of an edge is a non-negative measure of similarity between the two vertices. 
So higher weight implies greater similarity. 
%and $W$ be the weighted adjacency matrix, such that $W_{i,i} = 0$ and $w_{i,j}$ is the weight of the edge between $v_i \in V$ and $v_j \in V$. 
After then we get the normalized Laplacian matrix $L \in \mathbb{R}^{n \times n}$ as follows : 
\begin{equation}
L = D^{-1/2} A D^{-1/2}
\end{equation}
where $D \in \mathbb{R}^{n \times n}, d_{i,i} = \sum_{j} A_{i,j}$ be the diagonal degree matrix. 
%where $D$ is the diagonal degree matrix $d_{ii}$ is the detree of vertex $i$ defined as $d_{ii} = \sum_{j=1}^n$ and $W$ is the weighted adjacency matrix, such that $W_{ii} = 0$ and $w_{ij}$ is the weight of the edge between $v_i \in V$ and $v_j \in V$. 
%%\subsection{Algorithm}
%A spectral clustering builds embedded space from the eigenvectors corresponding to the $k$ eigenvalues before it does $k$-way clustering. 

We get the $k$ from $L$ with eigengap algorithm. 
Then we get the matrix $V \in \mathbb{R}^{n \times k}$ with the eigenvectors as columns.
Spectral clustering interprets the rows of $V$ as new data points $Z_i \in \mathbb{R}^k$ where $i \in \{1, \cdots, n\}$ 
%A spectral clustering builds embedded space from the eigenvectors corresponding to the $k$ eigenvalues before it does $k$-way clustering. 
%From $L$, we can get the eigenvalues and eigenvectors of Laplacian matrix, 
and we can apply known outlier detection techniques \cite{knorr00} to the reduced $\mathbb{R}^{n \times k}$ space to produce $k$ clusters. 

A spectral clustering has three issues. 
First of all, eigengap algorithm helps us to pick proper $k$ value, but no such algorithm can be perfect especially if there is too much noises. 
%We can check whether the result of eigengap is fine or not by threshold. 
%In this paper, I use $\sqrt{n}$ where $n$ is the number of data points as threshold on $k$. 
Secondly, the choice of similarity function has an important effect on the accuracy. 
Thirdly, we need to choose proper clustering method. 
%I follow $k$-class clustering approach as I mentioned before because I found recursive bipartite approach is not applicable for real data which has lots of noise. 

%we need to find the eigenvalues and eigenvectors of Laplacian matrix to decompose do this. 
%After we compute normalized graph Laplacian matrix, we can decompose the matrix. 
%After we find them, we can build embedded space from the eigenvectors corresponding to the $k$ eigenvalues given by eigengap algorithm. 
%After that, we can apply known outlier detection techniques \cite{knorr00} to the reduced $n \times k$ space to produce $k$ clusters. 
\begin{figure}[ht]
\label{fig:spectralclustering}
\begin{mdframed}
\begin{enumerate}
\item[Input] : Similarity matrix $S \in \mathbb{R}^{n \times n}$, number $k$ of clusters. 
\item[Step 1] : Construct the normalized graph Laplacian $L = D^{-1/2} A D^{-1/2}$.
%Construct the normalized graph Laplacian $L = D^{-1/2} A D^{-1/2} - W$, using the weighted $n \times n$ adjacency matrix $W$, where $w_{ij}$ is a function of $S_{ij}$ that gives non-negative weights. \\
\item[Step 2] : Find the $k$ eigenvectors $u_1, \cdots, u_k$ corresponding to the smallest $k$ eigenvalues that solve the generalized eigenvector problem $L u = \lambda D u$ 
\item[Step 3] : Let $U = R^{n \times k}$ be the matrix containing the eigenvectors $u_i$ as columns, and let $y_i \in R^k$ be the $i$th row of $U$.
\item[Step 4] : Cluster the points $(y_i)_{i=1,\cdots,n}$ in $\mathbb{R}^k$ using $k$-means clustering into clusters $C_1,\cdots,C_k$.
\item[Output] : Clusters $C_1, \cdots, C_k$
\end{enumerate}
\end{mdframed}
\caption{Overview of multiclass spectral clustering algorithm according to Shi}
\end{figure}
%A spectral clustering has three issues. 
%First of all, eigengap does not pick $k$ value correctly if there is too much noises. 
%We can check whether the result of eigengap is fine or not by threshold. 
%In this paper, I use $\sqrt{n}$ where $n$ is the number of data points as threshold on $k$. 
%Secondly, the choice of similarity function has an important effect on the accuracy. 
%Thirdly, we need to choose proper clustering method. 
%I follow $k$-class clustering approach as I mentioned before because I found recursive bipartite approach is not applicable for real data which has lots of noise. 
%For clustering method, there is two ways to do it. 
%First is a $k$-class clustering \cite{jianbo03} and the other is one by one iteratively until it makes $k$ clusters. 
%The problem of latter one is that it is too weak for a noise. 
%Also the number of cluster $k$ from its eigengap does not always best when the data have considerable noises. 
%Therefore I follow multi-class approach since iterated one-to-one clustering is not applicable. %in this problem because of the limitation of eigensolver's sensitivity. 
% I followed both way but one-by-one is sometimes really bad.
