\section{Spectral clustering}
% Basic
Spectral clustering algorithm is the graph cut algorithm involves taking top eigenvectors of an affinity matrix, which is transformed by calculating the similarity between objects in dataset, and then use a cluster algorithm.

The assuptions to the methods are that \begin{inparaenum}[\itshape a\upshape)]
\item minimizing normalized cut is equivalent to minimizing the probability that a random walk on the graph.
\item the larger the gap between $\lambda_{k}$ and $\lambda_{k+1}$, the closer the piecewise-constant eigenvectors will be.
\end{inparaenum}

\subsection{graph Laplacian}
Spectral clustering algorithms attemp to find $k$ subsets of vertices which have the property that vertices within a cluster are similar to each other than to vertices in other clusters. In order to find sets, spectral clustering makes the unnormalized Laplacian $n \times n$ matrix $L$.

$L = D - W$ where $D$ is the diagonal degree matrix $d_{ii}$ is the detree of vertex $i$ defined as $d_{ii} = \sum_{j=1}^n$ and $W$ is the weighted adjacency matrix, such that $W_{ii} = 0$ and $w_{ij}$ is the weight of the edge between $v_i \in V$ and $v_j \in V$.

Note that we are assuming that the graph is undirect which makes $W$ and $L$ symmetric and the weight of an edge is a non-negative measure of similiarity between the two vertices. 
Higher weights imply greater similarity. 

\subsection{Algorithm}
After we compute graph Laplacian matrix, we can decompose the matrix. 
In spectral approach, we need to find the eigenvalues and eigenvectors of Laplacian matrix to do this. 
After we find them, we can build embedded space from the eigenvectors corresponding to the $k$ eigenvalues given by eigengap algorithm. :w
However, eigengap does not work if there is too many noises. 
We can check whether the result of eigengap $k$ is too big or not.
After that, we can apply known outlier detection techniques\cite{knorr00} to the reduced $n \times k$ space to produce $k$ clusters. 
\begin{figure}[ht]
\begin{mdframed}
\begin{enumerate}
\item[Input] : Similarity matrix $S \in R^{n \times n}$, number $k$ of clusters. \\
\item[Step 1] : Construct the unnormalized graph Laplacian $L = D - W$, using the weighted $n \times n$ adjacency matrix $W$, where $w_{ij}$ is a function of $S_{ij}$ that gives non-negative weights. \\
\item[Step 2] : Find the $k$ eigenvectors $u_1, \cdots, u_k$ corresponding to the smallest $k$ eigenvalues that solve the generalized eigenvector problem $L u = \lambda D u$ \\
\item[Step 3] : Let $U = R^{n \times k}$ be the matrix containing the eigenvectors $u_i$ as columns, and let $y_i \in R^k$ be the $i$th row of $U$.\\
\item[Step 4] : Cluster the points $(y_i)_{i=1,\cdots,n}$ in $R^k$ using $k$-means clustering into clusters $C_1,\cdots,C_k$.\\
\item[Output] : clusters $A_1, \cdots, A_k$ where $A_k - {v_j|y_j \in C_i}$
\end{enumerate}
\end{mdframed}
\caption{Normalized spectral clustering according to Shi}
\end{figure}

\begin{figure}[htb2]
\begin{center}
\end{center}
\caption{Example of Spectral Clustering}
\label{fig:refSingleRobot1}
\end{figure}

For clustering method, there is two ways to do it. 
First is $k$-class clustering\cite{jianbo03} and the other is one by one iteratively until it makes $k$ clusters. 
The problem of latter one is that it is too weak for a noise. 
Therefore I follow multi-class approach. 
% I followed both way but one-by-one is sometimes really bad.
