\section{Network Connection Similarity}
\label{sec:connectionsimilarity}
A network connection is a connection between computers in the Internet to transmit and receive data. 
Connection data created by host computers is important to monitor of a network status.
Network connection similarity helps to measure the similarity between network connections, and is essential to construct clusters. 
%, and requires comparison methods that helps to measure the similarity between network connections. 
%Network connection similairy methods provide how the data points are similar each other and is essential to construct clusters. 
After reviewing the families of proposed schemes, I identified similarity and density approaches as a promising approach to solve the problem.
%In this report, we are gonig to measure similarity between network connections. 
%Since there is not only one normal state, I generated mixture models for normal connections for each protocols and attributes.
%so the problem of comparing similarity has been an important problem. %\cite{}
\newline
%In Section~\ref{subsec:problemformulation}, I describe the problem.\newline
In Section~\ref{subsec:normalabnormalsimilarity}, I propose new approaches to train similarity cost function.\newline
In Section~\ref{subsec:densitysimilarity}, I describe density similarity measurement which is relied on the way of representatives of the clusters and threshold.\newline
%In Section~\ref{subsec:learningsimilarity}, I describe how the algorithm learn mixture models with training set.\newline
%\subsection{Problem Formulation}
%\label{subsec:problemformulation}
%Given network connection data, we can learn normal and abnormal mixture models in order to measure similarity score and estimate a density of normal connections. 
%Anomalies can be detected by cluster algorithm based on affinity matrix which is computed by similarity score, or by comparing density of clusters against threshold. 
%The aim of the algorithm is to detect a set of anomalies and is not to classify them into types of anomalies. 
%
\subsection{Normal and Abnormal Network Connection Similarity}
\label{subsec:normalabnormalsimilarity}
I now describe how to train a similarity cost function for both normal and abnormal network connection data points. 
Let $\hat{x_i} = (score_{\text{normal}}, score_{\text{abnormal}})$ is a node that is two-dimensional 
where $score_{\text{normal}}$ is a similarity score to normal mixture models 
and $score_{\text{abnormal}}$ is a similarity score to abnormal mixture models. 
Then two nodes are similar if their cosine similarity are similar. 
%that is in spectral embedded space $\mathbb{R}^{n \times k}$ tranported from network connection data points $\mathbb{R}^{n}$ where $n$ is the number of data points and $k$ is the desirable number of clustering from eigengap algorithm. 
%% They compute the similarity score, log probability of each connections, under the model. 
%I found that cosine similarity shows better performance when to measure the similarity between two nodes $\hat{x_i}$ and $\hat{x_j}$. 
%A node $\hat{x_i} = (s_{\text{normal}}, s_{\text{abnormal}})$ is two-dimensional which is transported from 39-dimensional data point $x_i$. 
A similarity score is a real value and is defined as follows:

\begin{equation}
    score_j = \sum_{i=1}^{39} w_i \theta_i (x_j) 
\end{equation}
where $j \in \{1,\cdots,n\}$ is an index of data points, $x_j$ is $j$th data point in data points, $\theta_i$ is a mixture model's log-likelihood function of a data point's $i$'th attribute and $w_i$ is a weight of $i$th attribute based on its importance \cite{kayacik05}. 

%The similarity for given data point $x$ that is a normal or abnormal score $s = \sum_{i=1}^{39} w_i \theta_i$ 
To do this, we need to have mixture models for two classes - normal and abnormal, three protocols - TCP/IP, ICMP, UDP and 39 attributes as in Table~\ref{fig:preprocessing}. 
As a result the total number of mixture models that is required is 234 ($= 2 \cdot 3 \cdot 39$).
Each mixture model can be learned from the training set by EM algorithm, and it is guaranteed to converge to a local optimum on a given input. 
The log-likelihood function $\theta_i$ of a GMM in this case is :

\begin{equation}
    \theta_i(x_j) = \ln p(z_{i,j} \mid \pi, \mu, \Sigma) = \ln (\sum_{k=1}^K \pi_k N(z_{i,j} \mid \mu_k, \Sigma_k))
\end{equation}
where $k$ is an index of components of GMM, 
$\pi_k$ is a weight of $k$'th component of GMM, 
$\mu_k$ is a mean of $k$'th component of GMM, 
$\Sigma_k$ is a variance of $k$'th component of GMM, 
%$K$ is the number of components of GMM, 
and $z_{i,j}$ is a value of $j$th data point $x_j$'s $i$th attribute. 
For example, 38th column of the NSL-KDD dataset is "dst-host-rerror-rate" and 1st row data point's "dst-host-rerror-rate" is $0.05$. 
We call it as $z_{38,1} = 0.05$ in this case. 

%where $\theta_i$ is a log-likelihood probability calculated from a mixture model $m_i$ of all 39 attributes. 
%The log-likelihoods $\theta_1, \cdots, \theta_{39}$ are going to be weighted by $w_1, \cdots, w_{39}$ based on attribute's importance \cite{kayacik05}. 
%Each attribute has different correlation to the result \cite{olusola10}\cite{kayacik05}, so I give a different weight $w_i$ on them. 
%Also $(\theta_1, \cdots, \theta_{39})$ is called a score vector $v$. 
%As a result, we convert every 39-dimensional data point $x_i$ to 2-dimensional data point $\hat{x_i} = (s_{\text{normal}}, s_{\text{abnormal}})$ in the dataset. 
%So we are going to have nodes $X = (\hat{x_1}, \cdots, \hat{x_n})$. 
In short, a similarity score is simply a weighted-sum of all the elements of a score vector $v = (\theta_1(x_j), \cdots, \theta_{39}(x_j))$, 
and by using cosine similarity function, we can measure pairwise similarity between two transported data points. 
%we have a set of nodes $\hat{X} = (\hat{x_1}, \cdots, \hat{x_n})$ and 
%is going to be used to measure pairwise similarity 
%and it is going to be used to measure cosine similarity. 
%Learned $234(=2 \times 3 \times 39)$ Gaussian mixture models in total.
%\begin{itemize}
%\item 2 : one for normal, one for abnormal.
%\item 3 : each per each protocol e.g.) udp, icmp, tcp.
%\item 39 : for all attributes.
%Since the data fit the gaussian and a sufficient number of data points are available to learn the parameters of the model, the model can be learned.
%\begin{equation}
%    sim(V, V') = \frac{A \cdot B}{|A| |B|}
%\end{equation}
%

\subsection{Connection Density Similarity}
\label{subsec:densitysimilarity}
Another point of view on anomaly detection can be a comparison of densities. 
A data point is abnormal if its density differs from a density of known normal connections even though its cluster is classified to normal behaviour. 
%For anomaly detection, only density of the data point is compared with the density of known normal connections. 
If the density of one data point is higher than a supposed density in the region, it are classified as anomalies. 
So it allows detecting unknown anomalies which is similar to known normal connections. 
In contrast to the known normal and abnormal connection similarity, it only requires normal network connections that appear in the training set. 
%The We can adjust threshold if false-positive or false-negative is high. 
%This is illustrated in (Figure) where one cluster over the distribution and threshold. 
The density function $\Phi(\hat{x})$ is computed by the kernel density estimation \cite{ester96} as belows:
\begin{equation}
    \Phi(\hat{x_j}) = \dfrac{1}{n} \sum \limits_{i=1}^{n} K(\hat{x_j} - \hat{x_i})
\end{equation}
where $\hat{x_j}$ is a $j$th node, 
$n$ is the number of data points, 
$i$ is an index of data points, 
and $K$ is a kernel function. 
In this report, I use a standard normal distribution as a kernel function. 
%
%A density function only trained with normal network connections
%than that of threshold. 
%\begin{equation}
%    d = \sum_X \sum_Y 
%\end{equation}
%\subsubsection{Measuring similarity}
%\label{subsec:densitysimilarity}
%Gaussian mixture models from training set helps to measure those two similarity per each connection.
%\begin{itemize}
%\item Similarity to known normal behavior.
%\item Similarity to known abnormal behavior.
%\end{itemize}
